"""
Here we should care about all text preprocessing.
"""
from typing import List


def tokenize_text(text: str) -> List[str]:
    """
    A text should be given as input and split into tokens.
    Probably use spacy for this.
    """
    pass


def lemmatize_word(word: str) -> str:
    """
    A word should be given as input.
    Its lemma should be returned.
    Probably use spacy for this.
    """
    pass
